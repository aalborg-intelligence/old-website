---
title: "Funktioner af flere variable"
image: "images/maskine2.png"
description: En funktion kan godt afhænge af flere forskellige variable, og i det tilfælde taler man om en **funktion af flere variable**. Denne type af funktioner viser sig at spille en helt central rolle i rigtig mange metoder inden for kunstig intelligens. Derfor vil vi i denne note behandle de vigtigste begreber i forbindelse med funktioner af flere variable.
from: markdown+emoji
format:
    html:
      self-contained: true 
      toc: true
      toc-title: Indhold
      toc-location: left
      related-formats-title: "Andre formater"
      link-external-newwindow: true
#    pdf: default
reference-location: margin
editor_options: 
  chunk_output_type: console
crossref:
  fig-prefix: figur   # (default is "Figure")
  tbl-prefix: tabel    # (default is "Table")
  exm-prefix: eksempel
  thm-prefix: sætning
  sec-prefix: afsnit
  eq-prefix: ''
  fig-title: Figur
  exm-title: Eksempel
  thm-title: Sætning
  tbl-title: Tabel
label:
    fig: Figur
fig-cap-location: margin
tab-cap-location: margin
execute:
  echo: false
  warning: false
---

## Funktioner af én variabel

Fra gymnasiematematikken kender vi lineære funktioner, eksponentialfunktioner, potensfunktioner og en hel masse andre typer af funktioner. Fælles for dem er, at de alle afhænger af én variabel $x$. For eksempel de lineære funktioner:

$$
f(x)=a \cdot x+b.
$$
Grafen for $f$ består af alle de punkter $(x,y)$, hvor $y=f(x)$. Det er illustreret på @fig-graf_lin.

![Grafen for en lineær funktion $f(x)=ax+b$, hvor det er illustreret at et punkt $(x,y)$ ligger på grafen for $f$, hvis $y=f(x)$.](images/graf_lin.png){width=75% #fig-graf_lin}



Hvis funktionen $f$ tager et reelt tal som input og giver et reelt tal som output[^1], så skriver man

$$
f: \mathbb{R} \rightarrow \mathbb{R}.
$$ 
Du har måske også været vant til at se en \"maskine-metafor\", som illustreret på @fig-maskine.

[^1]: Hverken definitions- eller værdimængden for funktionen behøver ikke at være hele $\mathbb{R}$, men kan godt bare være en delmængde af $\mathbb{R}$.

![Funktionen $f$ illustreret som en maskine.](images/maskine.png){width=75% #fig-maskine}

Her er tanken, at man sender et $x$ ind i funktionsmaskinen, som så ved hjælp af forskriften for $f$ beregner funktionsværdi $y=f(x)$, som herefter bliver sendt ud af maskinen[^3]. 

[^3]: Faktisk er det ikke alle funktioner, som har en forskrift. For eksempel er der ikke nødvendigvis en forskrift for den funktion, hvis graf ses til venstre i @fig-grafer. Her vil man i stedet kunne aflæse en funktionsværdi for en given værdi af $x$. En anden mulighed er, at funktionsværdien skal findes i en tabel.

For at der overhovedet er tale om en funktion, skal det være sådan, at der til enhver værdi af $x$ svarer én og kun én funktionsværdi $f(x)$. Så hvis man for eksempel sender $x=2$ ind i maskinen, så må funktionen ikke nogle gange sende $y=5$ ud og andre gange $y=-3$. Hvis $x=2$ kommer ind, så skal det altid være den samme funktionsværdi, der kommer ud.

Grafisk er det illustreret på @fig-grafer. 

![Til venstre ses grafen for en funktion. Til højre ses en kurve, som ikke kan være grafen for en funktion. Ved den grønne linje ses for eksempel en $x$-værdi hvortil der svarer ikke kun én, men tre $y$-værdier.](images/grafer.png){width=75% #fig-grafer}


## Funktioner af to variable

Idéen med funktioner af to variable er en funktion $f$, som skal bruge to tal som input for at give et output. Vi kalder her de to inputværdier for $x$ og $y$. Her er et eksempel:

$$
f(x,y)= 2x^2-y^2+3xy+1.
$$

Grafen for $f$ består nu af alle de punkter $(x,y,z)$ i et tre-dimensionelt koordinatsystem, hvor $z=f(x,y)$. For eksempel er

$$
\begin{aligned}
f(1,-2)&=2\cdot 1^2-(-2)^2+3 \cdot 1 \cdot (-2)+1 \\
&=2-4-6+1=-7
\end{aligned}
$$
Det betyder, at punktet $P(1,-2,-7)$ ligger på grafen for $f$. Grafen for $f$ og punktet $P$ ses i figuren herunder.

{{< include _geogebra/_geogebra.qmd >}}

::: {#ggbApplet_funktion_af_to_variable}
:::
<!--
![Grafen for funktionen $f$ med forskrift $f(x,y)=2x^2-y^2+3xy+1$ samt punktet $P(1,-2,-7)$.](images/graf2.png){width=75% #fig-graf2} 
-->


Vi kan igen bruge \"maskine-metaforen\" for funktioner af to variable, som illustreret i @fig-maskine2.

![Funktionen $f$ af to variable illustreret som en maskine.](images/maskine2.png){width=75% #fig-maskine2}

For at vi kan tale om, at det er en funktion, kræver vi igen, at der til enhver værdi af $(x,y)$ svarer én og kun én funktionsværdi $z=f(x,y).$ Maskinen skal altså altid returnere den samme funktionsværdi for en given værdi af $(x,y)$.

Hvis en funktion $f$, som input kan tage en $(x,y)$-værdi, og som output giver en funktionsværdi $z$, skriver vi

$$
f: \mathbb{R}^2 \rightarrow \mathbb{R}.
$$ 


Her indikerer $2$-tallet altså, at der er tale om en funktion af *to* variable.

### Snitfunktioner og snitkurver

Vi vil nu for en funktion af to variable tage udgangspunkt i et konkret punkt $P(x_0, y_0, f(x_0,y_0))$.  Forestil dig at vi i $P$ laver et lodret snit ned gennem grafen for $f$ med en plan, som er parallel med $x$-aksen. Det er lidt ligesom, at lave et snit ned gennem en lagkage! Et eksempel er vist i @fig-graf_snitkurve_y. Den lyseblå plan svarer til den lagkagekniv, vi har skåret med, og den skærer altså lodret og er samtidig parallel med $x$-aksen.

Når man laver sådan et lodret snit med grafen for $f$, så kalder man skæringen mellem den lodrette plan (lagkagekniven) og grafen for $f$ for en *snitkurve*. Snitkurven er markeret med sort på @fig-graf_snitkurve_y. Den funktion, hvis graf svarer til snitkurven, kaldes for en *snitfunktion*. Forskriften for snitfunktionen fås ved, at vi fastholder $y$-værdien på  $y_0$ og lader $x$ varierer:

$$
g(x)=f(x,y_0).
$$
Vi kan også lave et lodret snit gennem $P(x_0,y_0,f(x_0,y_0))$, men hvor snittet i stedet er parallel med $y$-aksen -- se @fig-graf_snitkurve_x. Det svarer til, at vi fastholder $x$ på værdien $x_0$ og lader $y$ variere. Gør vi det fås snitfunktionen

$$
h(y)=f(x_0,y).
$$

Lad os se på eksemplet fra tidligere
$$
f(x,y)= 2x^2-y^2+3xy+1.
$$

Fastholder vi for eksempel $y$ på $-2$, får vi snitfunktionen

$$
\begin{aligned}
g(x) &=f(x,-2)=2x^2-(-2)^2+3 \cdot x \cdot (-2)+1 \\
&=2x^2-6x-3.
\end{aligned}
$$
Den tilhørende snitkurve svarer til skæringskurven mellem grafen for $f$ og planen med ligning $y=-2$, som ses i figuren herunder. Bemærk, at snitfunktionen $g$ er et andengradspolynomium, hvis graf er en parabel med grene, som vender opad, idet koefficienten til andengradsleddet er positiv.

::: {#ggbApplet_funktion_af_to_variable_snitfkt_g}
:::

<!-- ![Grafen for snitfunktionen $g$ med forskrift $g(x)=2x^2-6x-3$ markeret med sort. Den lyseblå plan er planen med ligning $y=-2$.](images/graf_snitkurve_y=-2.png){width=75% #fig-graf_snitkurve_y} -->


Fastholder vi derimod $x$ på $1$, får vi snitfunktionen

$$
\begin{aligned}
h(y)&=f(1,y)=2 \cdot 1^2 -y^2 +3 \cdot 1 \cdot y +1 \\
&=-y^2+3y+3.
\end{aligned}
$$
Her ser vi igen, at snitfunktionen er et andengradspolynomium, og grafen vil være en parabel med grene, der vender nedad. Snitkurven svarer til skæringskurven mellem grafen for $f$ og planen med ligning $x=1$, som vist herunder.

::: {#ggbApplet_funktion_af_to_variable_snitfkt_h}
:::

<!-- 
![Grafen for snitfunktionen $h$ med forskrift $h(y)=-y^2+3y+3$ markeret med sort. Den lyseblå plan er planen med ligning $x=1$.](images/graf_snitkurve_x=1.png){width=75% #fig-graf_snitkurve_x} -->

### Partielle afledede

Fra funktioner af én variabel ved vi, at nogle funktioner er differentiable. Det kan funktioner af to variable også være. Her defineres de såkaldte *partielle afledede* ved simpelthen af differentiere de to forskellige typer af snitfunktioner, som vi definerede ovenfor.

Lad os forklare det lidt nærmere. Snitfunktionen $g(x)=f(x,y_0)$ er jo en \"almindelig\" funktion af én variabel $x$. Hvis $g$ er differentiabel, så kalder man $g'(x)$ for den *partielle afledede af $f$ med hensyn til $x$*. Notationen for den partielle afledede af $f$ med hensyn til $x$ er som regel en af følgende:

$$
g'(x)=f_x'(x,y_0)=\frac{\partial f}{\partial x}=\frac{\partial}{\partial x}f(x,y_0),
$$

hvor $y_0$ her er en konstant.

Ser vi igen på vores eksempel, får vi

$$
f(x,y_0)= 2x^2-y_0^2+3xy_0+1.
$$
Så vil den partielle afledede med hensyn til $x$ være

$$
f_x'(x,y_0)=2 \cdot 2x-0 + 3 \cdot 1 \cdot y_0 + 0 = 4x+3y_0.
$$
Bemærk her, at $y_0^2$ differentieret bliver $0$, fordi $y_0$ er en konstant, og en konstant, som er lagt til, bliver som bekendt $0$, når vi differentierer. Når vi skal differentiere udtrykket $3xy_0$, så er $y_0$ en konstant, som er ganget på, og derfor lader vi den stå, når vi differentierer (ligesom vi lader $3$-tallet stå og differentierer $x$, som giver $1$).

Normalvis gider man ikke slæbe rundt på $y_0$ i ovenstående udtryk, fordi $y_0$ jo kan være en hvilken som helst værdi svarende til, at vi flytter det lodrette snit. Derfor skriver man som oftest bare 

$$
f_x'(x,y)=4x+3y.
$$


Når vi finder den partielle afledede med hensyn til $x$, kan vi altså gøre det helt generelt, hvor vi bare tænker på $y$ som en konstant. 

Tidligere fastholdt vi $y$ på værdien $-2$. Indsætter vi $y=-2$ i ovenstående udtryk for $f_x'(x,y)$ får vi

$$
f_x'(x,-2)=4x+3 \cdot (-2)=4x-6.
$$
Men vi kunne lige så godt have differentieret snitfunktionen

$$
g(x)=f(x,-2)=2x^2-6x-3.
$$
Gør vi det, får vi
$$
g'(x)=4x-6,  
$$
der heldigvis svarer til udtrykket for $f_x'(x,-2)$, som vi netop har fundet.

Helt tilsvarende definerer vi den *partielle afledede af $f$ med hensyn til $y$* ved at differentiere snitfunktionen $h(y)$ (såfremt denne snitfunktion er differentiabel):

$$
h'(y)=f_y'(x_0,y)=\frac{\partial f}{\partial y}=\frac{\partial}{\partial y}f(x_0,y).
$$
Ser vi igen på
$$
f(x_0,y)= 2x_0^2-y^2+3x_0y+1,
$$
så vil den partielle afledede med hensyn til $y$ være

$$
f_y'(x_0,y)=0-2y+3x_0\cdot 1+0=3x_0-2y.
$$

Her er $x_0$ en konstant, og derfor er $2x_0^2$ differentieret $0$, og $3x_0y$ differentieret med hensyn til $y$ bliver $3x_0$.

Igen vil vi som oftest bare skrive

$$
f_y'(x,y)=3x-2y.
$$

### Grafisk betydning af de partielle afledede

For en funktion $f$ af én variabel ved vi, at hvis $f$ er differentiabel i $x_0$, så vil $f'(x_0)$ svarer til hældningen for tangenten til grafen for $f$ i punktet $(x_0,f(x_0))$.

Vi kan nu udlede en tilsvarende grafisk betydning af de partielle afledede. Vi ser igen på snitfunktionen $g(x)$, hvor $y$ er fastholdt på $y_0$:

$$
g(x)=f(x,y_0)
$$

Den partielle afledede med hensyn til $x$ er så

$$
g'(x)=f_x'(x,y_0).
$$
Men nu må $g'(x_0)$ være hældningen for tangenten til grafen for snitfunktionen $g$ i punktet $(x_0,g(x_0))$. 

Lad os illustrere det med vores eksempel hvor $f(x,y)= 2x^2-y^2+3xy+1$. Her er

$$
f_x'(x,y)=4x+3y.
$$
Vi fandt tidligere, at $P(1,-2,-7)$ ligger på grafen for $f$. Ser vi på snitfunktionen $g(x)=f(x,-2)$, så ligger $P$ altså også på den tilsvarende snitkurve. <!-- -- se også @fig-graf_snitkurve_y --> Udregner vi $g'(1)=f_x'(1,-2)$, får vi

$$
f_x'(1,-2)=4 \cdot 1 + 3 \cdot (-2) = 4-6=-2
$$

Det betyder, at hvis vi tegner tangenten til snitkurven i $P$, så vil denne tangent have en hældning på $-2$, som illustreret i figuren herunder.

::: {#ggbApplet_funktion_af_to_variable_snitfkt_g_med_tangent}
:::

<!-- ![Tangenten (stiplet linje) til grafen for snitfunktionen $g$ med forskrift $g(x)=2x^2-6x-3$ i punktet $P(1,-2,-7)$ har en hældning på $-2$.](images/graf_snitkurve_partialx.png){width=75% #fig-graf_snitkurve_partialx} -->


Helt tilsvarende kan vi fortolke den partielle afledede af $f$ med hensyn til $y$ i punktet $P$. Vi fandt, at


$$
f_y'(x,y)=3x-2y.
$$

Så i $(1,-2)$ har vi

$$
f_y'(1,-2)= 3 \cdot 1 - 2 \cdot (-2)=3+4=7
$$
Altså vil tangenten til snitkurven hørende til snitfunktionen $h(y)=f(1,y)$ have en tangenthældning på $7$ i punktet $(-2,h(-2))$. Det er vist i figuren herunder.

::: {#ggbApplet_funktion_af_to_variable_snitfkt_h_med_tangent}
:::

<!-- ![Tangenten (stiplet linje) til grafen for snitfunktionen $h$ med forskrift $h(y)=-y^2+3y+3$ i punktet $P(1,-2,-7)$ har en hældning på $7$.](images/graf_snitkurve_partialy.png){width=75% #fig-graf_snitkurve_partialy} -->


De partielle afledede svarer altså til tangenthældninger på snitkurver, som er fremkommet ved at finde skæringskurven mellem grafen for $f$ og en plan med ligning $x=x_0$ eller grafen for $f$ og en plan med ligning $y=y_0$. 

Man kan også bestemme tangenthældninger for mere generelle snitkurver, som fremkommer ved, at man finder skæringskurven mellem grafen for $f$ og en plan med ligning $ax+by+c=0$ (som står vinkelret på $xy$-planen). Differentierer man de tilhørende snitfunktioner, kalder man de afledede for *retningsafledede*. Det kan du læse meget mere om [her](../retningsafledede/retningsafledede.qmd).

For funktioner af flere variable kan man også bestemme en ligning for en såkaldt **tangentplan**. Det kan du læse mere om [her](tangentplaner.qmd).


### Gradienten og betydningen af denne

Man definerer *gradienten* for en funktion $f$ af to variable, som den vektor hvis koordinater svarer til de partielle afledede. Gradienten skrives $\nabla f (x,y)$. Det vil sige, at

$$
\nabla f(x,y) = 
\begin{pmatrix}
f_x'(x,y) \\ f_y'(x,y)
 \end{pmatrix}.
$$

Vi ser her, at gradienten er en to-dimensionel vektor. Det betyder, at man kan tegne en repræsentant for vektoren i $xy$-planen.

Lad os regne lidt på det i vores eksempel. Vi husker, at $f(x,y)=2x^2-y^2+3xy+1$ og vi fandt ovenfor, at
$$
f_x'(1,-2)=-2
$$
og

$$
f_y'(1,-2)=7.
$$
Det vil sige, at
$$
\nabla f(1,-2) = 
\begin{pmatrix}
-2 \\ 7
 \end{pmatrix}
$$

Tegner vi en repræsentant for denne vektor i $xy$-planen med udgangspunkt i punktet $Q(1,-2,0)$ (som er projektionen af $P(1,-2,-7)$ op på $xy$-planen) ses resultatet i app'en herunder. Hvis du drejer lidt rundt på grafen, kan du se, at gradienten angiver en retning i $xy$-planen. Laver man et snit med en lodret plan gennem $P$, som peger i gradientens retning fås den snitkurve, som er indtegnet med sort.



::: {#ggbApplet_funktion_af_to_variable_med_snitkurve_i_gradietens_retning}
:::
<!--
![En repræsentant for vektoren $\nabla f(1,-2)$ er tegnet med udgangspunkt i $P(1,-2,7)$'s projektion på $xy$-planen.](images/graf_gradient.png){width=75% #fig-graf_gradient}
-->


Det er jo alt sammen fint nok, men hvad skal man mon bruge det til? Det er nemmest at forklare, hvis du forestiller dig, at grafen for $f$ er et landskab, hvor du står i punktet $P(x_0,y_0,f(x_0,y_0))$. Så viser det sig, at gradienten $\nabla f(x_0,y_0)$ peger i den retning, hvor funktionsværdien vokser mest. Det vil altså sige, at hvis du står i $P$ og gerne vil gå allermest opad bakke, så skal du gå i gradientens retning. Det svarer derfor til at følge den snitkurve, som er markeret på app'en ovenfor. Hvis du drejer rundt på grafen i app'en, kan du måske få en fornemmelse af, at snitkurven netop angiver den \"sti\", man skal følge, hvis man i punktet $P$ vil gå mest opad bakke. 

Omvendt vil det også være sådan, at hvis du vil bevæge dig allermest nedad bakke, så skal du gå i retningen $-\nabla f(x_0,y_0)$ (det svarer til, at du lige har vendt dig rundt $180^{\circ}$ i forhold til den retning, som gradienten peger i). Beviset for at det forholder sig sådan, kan du læse [her](../retningsafledede/retningsafledede.qmd). 

Det er vigtigt her at bemærke, at det med at gå i gradientens positive eller negative retning for at gå allermest opad eller nedad bakke kun gælder i nærheden af punktet $P$. Man kalder det derfor også for en \"lokal\" egenskab. Det betyder, at hvis man er kommet lidt væk fra punktet $P$, så må man beregne en ny gradient i det nye punkt, man står i, og denne gradient vil nu formentlig pege i en helt ny retning. Det vil sige, at vi i dette nye punkt nu skal gå langs en ny \"sti\" for at gå allermest opad eller nedad bakke.

Fint nok tænker du måske, men hvem gider at gå allermest opad bakke? Se det er her, at det fine kommer ind i billedet og grunden til, at vi overhovedet gider tale om funktioner er flere variable i forbindelse med kunstig intelligens. I rigtig mange metoder inden for kunstig intelligens skal \"den kunstige intelligens\" trænes for at blive god[^2]. Eksempler er [kunstige neurale netværk](../neurale_net/neurale_net.qmd), [perceptroner](../perceptron/perceptron.qmd) og [sigmoid neuroner](sigmoid_neuron/sigmoid_neuron.qmd), som du kan læse meget mere om senere. 

Ser vi for eksempel på et kunstigt neuralt netværk, så kan det grundlæggende ingenting til at starte med. Men så giver man netværket nogle *træningsdata*, så det gradvist kan blive bedre. Det er her, at man siger, at man *træner* netværket. Det foregår ved, at man definerer en såkaldt *tabsfunktion*. Den måler basalt set, hvor godt netværket er lige nu. Man ønsker at tabsfunktionen skal minimeres (lille tab = godt netværk). Det kan være en meget kompliceret opgave, hvis man analytisk skal bestemme minimum for en sådan tabsfunktion, og det er lige præcis her, at gradienten kommer ind i billedet. Man starter nemlig et tilfældigt sted på grafen for tabsfunktionen (svarende til at man stiller sig i et punkt $P$ på grafen for en funktion $f$). Så udregner man gradienten, og da man gerne vil finde minimum for tabsfunktionen, så går man et lille skridt i den negative gradients retning. For vi ved jo netop, at det er den retning, vi skal gå i, hvis vi gerne vil gå i den retning, hvor funktionsværdien falder mest (og det er jo smart, hvis man gerne vil ende i et minimum). Så står man i et nyt punkt på grafen, udregner gradienten i det nye punkt og går så et lille skridt i denne gradients negative retning. Sådan fortsætter man, indtil man har fundet minimum eller noget, der er tæt på (eller måske bare \"godt nok\" -- det kan nemlig godt være et lokalt minimum, man rammer ind i)[^4]. Se det er faktisk *the backbone* i nogle af de mest populære og moderne metoder inden for kunstig intelligens, som anvendes i dag!

[^2]: Og nu tænker du nok, at det er derfor, at man skal gå opad bakke, men det er alligevel ikke helt sådan det forholder sig :blush:.

[^4]: Når et ekstremum for en funktion bestemmes på denne måde, kalder man det for en *numerisk* metode. Det står i modsætning til at bestemme ekstrema for en funktion *analytisk*. Bestemmes ekstrema analytisk sættes den afledede funktion (eller de partielle afledede funktioner) lig med $0$, og herefter finder man ud af, om der er tale om et maksimum eller minimum (eller eventuelt ingen af delene). Nogle gange er det enten beregningsmæssigt tungt eller helt umuligt at løse de ligninger, som det kræves for at finde ekstrema analytisk. Derfor er de numeriske metoder smarte, også selvom man ikke nødvendigvis finder det præcise ekstrema, men kun et punkt som er tæt på. Du kender faktisk allerede en anden numeriske metode nemlig *Newton-Raphsons* metode, som bruges til at bestemme en funktions nulpunkter (det vil sige, at løse ligningen $f(x)=0$).  


## Funktioner af flere variable

Vi har lige redegjort for, at gradienten er helt central i forbindelse med minimering af tabsfunktioner. Men faktisk vil det være sådan, at tabsfunktioner i virkeligheden ikke kun afhænger af to, men derimod af millioner af variable! Derfor har man helt generelt brug for at se på funktioner af flere variable end to. Vi kalder input-værdierne til funktionen $(x_1, x_2, \dots , x_n)$, og man taler så om en *funktion* af $n$ variable, hvis der for enhver værdi af $(x_1, x_2, \dots , x_n)$ svarer én og kun én funktionsværdi:
$$
y = f(x_1, x_2, \dots, x_n).
$$

Man skriver derfor

$$
f: \mathbb{R}^n \rightarrow \mathbb{R},
$$ 
hvor $n$'et her tydeliggør, at der er tale om en funktion af $n$ variable. 

Grafen for $f$ består af alle de punkter $(x_1, x_2, \dots, x_n,y)$ i et $(n+1)$-dimensionalt koordinatsystem, hvor $y = f(x_1, x_2, \dots, x_n)$. Koordinatsystemer på $4$ dimensioner eller flere er svære at rumme i vores $3$-dimensionelle verden, så af den grund behøver vi ikke at bekymre os om, hvordan grafen for $f$ ser ud. 

De partielle afledede defineres helt som før. Når man for eksempel skal finde den partielle afledede med hensyn $x_1$, så betragtes $x_2, x_3, \dots, x_n$ som konstanter, og man differentierer den tilsvarende snitfunktion med hensyn til $x_1$. De partielle afledede betegnes med

$$
\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n}.
$$
Gradienten defineres som en naturlig udvidelse af den tidligere definition

$$
\nabla f(x,y) = 
\begin{pmatrix}
\frac{\partial f}{\partial x_1} \\ 
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
 \end{pmatrix}.
$$

Helt analogt til før viser det sig, at gradienten peger i den retning, hvor funktionsværdien vokser mest. Og minus gradienten peger så altså i den retning, hvor funktionsværdien aftager mest. 

Derfor kan vi igen gå i den negative gradients retning, når vi gerne vil minimere en tabsfunktion. Denne metode kaldes for øvrigt for *gradientnedstigning* eller på engelsk *gradient descent*.

## Videre læsning

- Forberedelsesmaterialet om \"Funktioner af to variable\" fra 2013. Ministeriet for børn og undervisning.
