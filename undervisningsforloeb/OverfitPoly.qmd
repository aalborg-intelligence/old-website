---
title: "Overfitting og krydsvalidering med polynomiel regression"
description-meta: 'Introduktion til begreberne overfitting og krydsvalidering vha. polynomiel regression'
image: "OverfitPoly_filer/3gradsregression6af8udenkode.png"

format:
    html: 
      self-contained: true
      toc: true
      toc-title: Indhold
      toc-location: left
      link-external-newwindow: true
from: markdown+emoji
reference-location: margin
categories:
  - B-niveau
crossref:
  fig-prefix: figur   # (default is "Figure")
  tbl-prefix: tabel    # (default is "Table")
  exm-prefix: eksempel
  thm-prefix: sætning
  sec-prefix: afsnit
  eq-prefix: ''
  fig-title: Figur
  exm-title: Eksempel
  thm-title: Sætning
  tbl-title: Tabel
label:
    fig: Figur
fig-cap-location: margin
tab-cap-location: margin
tbl-cap-location: margin
execute:
  echo: false
  warning: false
---


::: {.callout collapse="true" appearance="minimal"}
### Forudsætninger og tidsforbrug
Forløbet kræver kendskab til:

+ Polynomiel regression


**Tidsforbrug:** ca. 90 minutter.

:::

::: {.purpose}

### Formål

I dette forløb skal du lære om begreberne overfitting og krydsvalidering ved at lave en række opgaver med brug af polynomiel regression. Du kan læse mere om dette i denne note 
[krydsvalidering](../materialer/krydsvalidering/krydsvalidering.qmd)
, men kendskab til notens indhold er ikke en forudsætning. 

:::

## Introduktion

Man vil ofte gerne ud fra kendte observationer i en stikprøve kunne forudsige værdier af fremtidige observationer fra den population, som stikprøven er fra. Dette kaldes prediktion. I virkeligheden vil man ofte have en stikprøve med 100 eller flere observationer, men for at undgå alt for mange beregninger, nøjes vi her med 8, selvom det i praksis er alt for lidt.

I dette eksempel vil vi se på populationen ”danske gymnasieelever”, hvor vi, indrømmet fjollet, vil undersøge, om der en sammenhæng mellem den uafhængige variabel ”antal biografbesøg det seneste år” og den afhængige variabel ”antal venner på de sociale medier”. Vi lader som om, at vi har indsamlet en stikprøve med 8 gymnasielever med følgende resultat:

$Bio = [1, 2, 3, 4, 5, 6, 7, 8]$

$Venner = [14, 27, 11, 19, 27, 24, 12, 39]$

Vi ønsker ud fra disse data at opstille en model, som for nye observationer kan forudsige, hvor mange venner på de sociale medier en gymnasieelev har, hvis man kender antal biografbesøg.

Når man opstiller en model, kan man nogle gange bygge på en forventning eller fysisk model, men andre gange har man som udgangspunkt ikke nogen bestemt ide, hvilket er tilfældet her. Vi vil derfor forsøge at modellere data vha. et polynomium, hvor vi så skal undersøge, hvilken grad af polynomiet, der ser ud til at kunne klare opgaven bedst. Her ses f.eks. resultatet af regression med et 3. gradspolynomium.

![3. gradsregression på alle 8 datapunkter.](OverfitPoly_filer/3gradsregression.png){#fig-3grads}

## Opgave 1
*	Overvej og diskuter hvad den højeste grad er, man kan lave polynomiel regression med, når der er 8 punkter? 
*	Overvej tilsvarende, hvad den mindste grad er?


## Opgave 2
*	Lav lineær regression samt polynomiel regression fra 2. grads til 7. grads på stikprøvens data. 
*	Tegn for hver regression punktplottet og grafen for resultatet af regression sammen, hvis dit CAS- værktøj ikke gør det af sig selv.  
*	Hvilket polynomium passer bedst til de 8 punkter?

Svaret bør ikke være overraskende. Desto højere grad af polynomium, desto bedre kan grafen tilpasse sig punkterne. Når graden bliver antallet af punkter minus 1, altså her graden 7, passer grafen perfekt til alle punkterne. Men betyder det så også, at det fundne 7. gradspolynomium passer godt til fremtidige observationer og dermed til at prediktere, hvor mange venner på de sociale medier andre elever har ud fra antal biografbesøg? Det vil vi undersøge nærmere i resten af forløbet.

## Krydsvalidering

Den metode, vi vil anvende, kaldes for krydsvalidering. Vi vil lave regressionen ud fra 6 af de 8 punkter og beregne, hvor godt resultatet heraf passer med de sidste 2 punkter – vi lader så at sige som om, at vi skal prediktere værdien for de 2 sidste punkter. Det vil vi gøre 4 gange – første gang anvendes punkt 1 og 2 ikke i regressionen, anden gang anvendes punkt 3 og 4 ikke, så anvendes 5 og 6 ikke og til sidst anvendes 7 og 8 ikke. 

Her er vist et eksempel, lavet med Maple, men det samme kan gøres i andre værktøjer, hvor punkt 3 og 4 er fjernet inden regressionen, og den lodrette afstand fra hver af de to punkter til grafen er beregnet.

![3. gradsregression på 6 af de 8  datapunkter.](OverfitPoly_filer/3gradsregression6af8.png){#fig-3grads6af8}

Som det ses af figuren, ligger det 3. punkt ca. 24 under grafen fra regressionen uden punkt 3 og 4, mens det 4. punkt ligger ca. 13 under. Beregningerne viser de præcise værdier.


## Opgave 3
Hvis I er flere grupper, kan I nu passende dele opgaven op, så grupperne laver beregningerne for forskellige gradtal, gruppe 1 laver for 1. grads, gruppe 2 for 2. grads osv. op til 5. grads. (højere er ikke muligt, da der kun er 6 punkter). Hvis din gruppe er hurtigt færdig, så lav endeligt beregningerne for flere grader.

*	Lav 1., 2. 3., 4. eller 5. gradsregression ud fra 6 af punkter, idet du ikke medtager de to første punkter.
*	Tegn grafen fra regression og alle 8 punkter i samme koordinatsystem for visuelt at illustrere, hvor godt eller skidt de 2 punkter er predikteret af regressionen, som vist i eksemplet.
*	Beregn den lodrette afstand mellem grafen og hver af de 2 punkter, som ikke var med i regression. Disse kaldes for residualer, så lad os kalde dem for $r_1$ og $r_2$. Skriv værdierne i en tabel som den nedenfor.

    | Residual	| $r_1$ |	$r_2$ |	$r_3$ |	$r_4$ |	$r_5$ |	$r_6$ |	$r_7$ |	$r_8$ |
    |:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
    | Værdi | | | | | | | | |
    : {.bordered}

*	Gentag regression og beregninger for samme grad, hvor det blot er de to næste punkter, der ikke er med i regression, så de to næste og endeligt de to sidste.
*	Til sidst skal der beregnes en samlet afstand for alle 8 punkter. Det gøres ved at kvadrere hver enkelt værdi og beregne summen. Altså ${r_1}^2+{r_2}^2+ {r_3}^2+{r_4}^2+{r_5}^2+{r_6}^2+{r_7}^2+{r_8}^2$. 

## Opgave 4
*	Sammenlign de samlede afstande for 1. grads, 2. grads osv. Hvilken grad er bedst til at prediktere fremtidige værdier?

## Overfitting

Det fænomen, som dette forløb illustrerer, kaldes for overfitting. Ved at tilpasse modellen for godt til observationerne, får man ikke lavet en passende generel model, men derimod en model til netop disse punkter. Så selvom et 7. gradspolynomium passer perfekt til de 8 punkter, så viste et 2. gradspolynomium sig at være bedst til prediktion ifølge undersøgelsen med krydsvalidering. 


